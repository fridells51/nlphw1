{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e51548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                              claim  \\\n",
      "0   0  0-dimensional biomaterials lack inductive prop...   \n",
      "1   2  1 in 5 million in UK have abnormal PrP positiv...   \n",
      "2   4  1-1% of colorectal cancer patients are diagnos...   \n",
      "3   6  10% of sudden infant death syndrome (SIDS) dea...   \n",
      "4   9  32% of liver transplantation programs required...   \n",
      "5  10  4-PBA treatment decreases endoplasmic reticulu...   \n",
      "6  11  4-PBA treatment raises endoplasmic reticulum s...   \n",
      "7  12  40mg/day dosage of folic acid and 2mg/day dosa...   \n",
      "8  14                   5'-nucleotidase metabolizes 6MP.   \n",
      "9  15  50% of patients exposed to radiation have acti...   \n",
      "\n",
      "                                            evidence cited_doc_ids  \n",
      "0                                                 {}    [31715818]  \n",
      "1  {'13734012': [{'sentences': [4], 'label': 'CON...    [13734012]  \n",
      "2                                                 {}    [22942787]  \n",
      "3                                                 {}     [2613775]  \n",
      "4  {'44265107': [{'sentences': [15], 'label': 'SU...    [44265107]  \n",
      "5                                                 {}    [32587939]  \n",
      "6                                                 {}    [32587939]  \n",
      "7  {'33409100': [{'sentences': [8], 'label': 'SUP...    [33409100]  \n",
      "8                                                 {}      [641786]  \n",
      "9                                                 {}    [22080671]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "DATA_DIR = Path('./scifact_data/data')\n",
    "df_train = pd.read_json(DATA_DIR / \"claims_train.jsonl\", lines=True)\n",
    "df_dev   = pd.read_json(DATA_DIR / \"claims_dev.jsonl\",   lines=True)\n",
    "df_test  = pd.read_json(DATA_DIR / \"claims_test.jsonl\",  lines=True)\n",
    "df_corpus= pd.read_json(DATA_DIR / \"corpus.jsonl\",       lines=True)\n",
    "\n",
    "print(df_train.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f51e044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings...\n",
      "\n",
      "Evaluating on train set...\n",
      "  MRR@1: 0.5884 | MAP@1: 0.5884\n",
      "  Valid queries: 809/809\n",
      "  Used cited_doc_ids fallback for 304 claims\n",
      "  MRR@10: 0.6808 | MAP@10: 0.6771\n",
      "  Valid queries: 809/809\n",
      "  Used cited_doc_ids fallback for 304 claims\n",
      "  MRR@50: 0.6851 | MAP@50: 0.6821\n",
      "  Valid queries: 809/809\n",
      "  Used cited_doc_ids fallback for 304 claims\n",
      "0 claims missing from embeddings\n",
      "0 claims with no relevant documents\n",
      "\n",
      "Train Set:\n",
      "  MRR@1: 0.5884\n",
      "  MAP@1: 0.5884\n",
      "  MRR@10: 0.6808\n",
      "  MAP@10: 0.6771\n",
      "  MRR@50: 0.6851\n",
      "  MAP@50: 0.6821\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the pickled embeddings\n",
    "print(\"Loading embeddings...\")\n",
    "with open(\"scifact_evidence_embeddings.pkl\", \"rb\") as f:\n",
    "    doc_embeddings = pickle.load(f)\n",
    "\n",
    "with open(\"scifact_claim_embeddings.pkl\", \"rb\") as f:\n",
    "    claim_embeddings = pickle.load(f)\n",
    "\n",
    "DATA_DIR = Path('./scifact_data/data')\n",
    "df_train = pd.read_json(DATA_DIR / \"claims_train.jsonl\", lines=True)\n",
    "# we only have embeddings for train\n",
    "#df_dev = pd.read_json(DATA_DIR / \"claims_dev.jsonl\", lines=True)\n",
    "#df_test = pd.read_json(DATA_DIR / \"claims_test.jsonl\", lines=True)\n",
    "\n",
    "doc_id_to_idx = {}\n",
    "doc_embeddings_matrix = []\n",
    "doc_ids = []\n",
    "\n",
    "for i, (doc_key, embedding) in enumerate(doc_embeddings.items()):\n",
    "    doc_id, abstract = doc_key\n",
    "    doc_id_to_idx[str(doc_id)] = i\n",
    "    doc_embeddings_matrix.append(embedding)\n",
    "    doc_ids.append(str(doc_id))\n",
    "\n",
    "doc_embeddings_matrix = np.array(doc_embeddings_matrix).astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatL2(doc_embeddings_matrix.shape[1])\n",
    "index.add(doc_embeddings_matrix)\n",
    "\n",
    "claim_id_to_embedding = {}\n",
    "for (claim_id, claim_text), embedding in claim_embeddings.items():\n",
    "    claim_id_to_embedding[claim_id] = embedding\n",
    "\n",
    "def average_precision_at_k(retrieved_ids, relevant_ids_set, k):\n",
    "    hits = 0\n",
    "    precisions = []\n",
    "    for i, doc_id in enumerate(retrieved_ids[:k], start=1):\n",
    "        if doc_id in relevant_ids_set:\n",
    "            hits += 1\n",
    "            precisions.append(hits / i)\n",
    "    if not relevant_ids_set:\n",
    "        return 0.0\n",
    "    return sum(precisions) / min(len(relevant_ids_set), k)\n",
    "\n",
    "\n",
    "def evaluate_retrieval(df_eval, split_name=\"eval\"):\n",
    "    \n",
    "    print(f\"\\nEvaluating on {split_name} set...\")\n",
    "    k_values = [1, 10, 50]\n",
    "    results = {}\n",
    "    \n",
    "    notinembedding = 0\n",
    "    notrelevant = 0\n",
    "\n",
    "    for k in k_values:\n",
    "        valid_queries = 0\n",
    "        empty_evidence_fallbacks = 0\n",
    "        reciprocal_ranks = []\n",
    "        avg_precisions = []\n",
    "\n",
    "        for _, row in df_eval.iterrows():\n",
    "            claim_id = row['id']\n",
    "\n",
    "            # skip if no embedding for this claim\n",
    "            if claim_id not in claim_id_to_embedding:\n",
    "                notinembedding += 1\n",
    "\n",
    "                continue\n",
    "\n",
    "            # Build the set of relevant doc ids (strings)\n",
    "            # Some have evidence, but some don't so use cited_doc_ids as fallback\n",
    "            evidence = row.get('evidence', {}) or {}\n",
    "            if isinstance(evidence, dict) and len(evidence) > 0:\n",
    "                relevant_ids = {str(doc_id) for doc_id in evidence.keys()}\n",
    "            else:\n",
    "                empty_evidence_fallbacks += 1\n",
    "                cited = row.get('cited_doc_ids', []) or []\n",
    "                relevant_ids = {str(doc_id) for doc_id in cited}\n",
    "\n",
    "            # Keep only relevant docs that exist in our index\n",
    "            relevant_ids = {doc_id for doc_id in relevant_ids if doc_id in doc_id_to_idx}\n",
    "            if not relevant_ids:\n",
    "                notrelevant += 1\n",
    "                continue\n",
    "\n",
    "            valid_queries += 1\n",
    "            claim_embedding = np.asarray([claim_id_to_embedding[claim_id]], dtype='float32')\n",
    "\n",
    "            # search\n",
    "            distances, indices = index.search(claim_embedding, k)\n",
    "            retrieved_doc_ids = [doc_ids[idx] for idx in indices[0]]\n",
    "\n",
    "            # rr@k\n",
    "            rr = 0.0\n",
    "            for rank, did in enumerate(retrieved_doc_ids, start=1):\n",
    "                if did in relevant_ids:\n",
    "                    rr = 1.0 / rank\n",
    "                    break\n",
    "            reciprocal_ranks.append(rr)\n",
    "\n",
    "            # AP@k\n",
    "            ap = average_precision_at_k(retrieved_doc_ids, relevant_ids, k)\n",
    "            avg_precisions.append(ap)\n",
    "\n",
    "        mrr = float(np.mean(reciprocal_ranks)) if reciprocal_ranks else 0.0\n",
    "        map_k = float(np.mean(avg_precisions)) if avg_precisions else 0.0\n",
    "        results[f\"MRR@{k}\"] = mrr\n",
    "        results[f\"MAP@{k}\"] = map_k\n",
    "        print(f\"  MRR@{k}: {mrr:.4f} | MAP@{k}: {map_k:.4f}\")\n",
    "\n",
    "        print(f\"  Valid queries: {valid_queries}/{len(df_train)}\")\n",
    "        print(f\"  Used cited_doc_ids fallback for {empty_evidence_fallbacks} claims\")\n",
    "    print(notinembedding, \"claims missing from embeddings\")\n",
    "    print(notrelevant, \"claims with no relevant documents\")\n",
    "    return results\n",
    "\n",
    "train_results = evaluate_retrieval(df_train, \"train\")\n",
    "\n",
    "\n",
    "print(\"\\nTrain Set:\")\n",
    "for metric, score in train_results.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
